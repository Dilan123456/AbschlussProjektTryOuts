Manual for the student reviewing my project:

To run the application you will have to create a python environment using the requirements.txt file.

To start the web-application you have to switch to the directory of the app folder and run the project with the following command:
flask run

In the Folder images_Kopie you will find 3 Subfolders. The Subfolder labeled as test contains images you can use to test the 
application and response. The test-subfolder contains 30 Subfolders each one representing a category of waste. 
Every Category contains 350 images to choose from, which you can upload on to the web application.


Motivation Behind the Project
In todays world, waste management is an increasingly critical issue as we grapple with the environmental impacts of improperly disposed waste. 
The problem at hand is the inefficient and often incorrect classification of waste, leading to significant challenges in recycling processes and 
overall waste management. Misclassification of trash can lead to contamination of recyclable materials, increased landfill waste, and environmental 
pollution.

Problem
The primary problem this project addresses is the need for an accurate, automated system to classify waste into correct categories. 
The manual sorting of waste is not only time-consuming and labor-intensive but also prone to human error. An effective solution to this problem can 
significantly enhance recycling efficiency and contribute to a more sustainable environment.

Motivation
The motivation behind developing a neural network for image-based trash classification stems from the urgent need to improve waste management practices globally. 
By leveraging machine learning and neural networks, we can create a system that assists individuals and organizations in accurately sorting waste, 
thereby reducing the strain on recycling facilities and minimizing the environmental footprint.

Relevance of the Project
This project is highly relevant in the context of global environmental sustainability efforts. Effective waste classification and management are crucial for reducing pollution, 
conserving natural resources, and ensuring the longevity of our planet. By providing an accessible tool that utilizes advanced machine learning techniques, 
this project empowers users to contribute to these sustainability goals.

Moreover, this project aligns with the broader push towards smart cities and the integration of technology into everyday life to solve practical problems. 
It also showcases the potential of artificial intelligence in creating impactful solutions for environmental challenges, promoting a cleaner, greener future for all.


Dataset
Link to the Dataset:https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification?resource=download-directory&select=images


Interpretation and Validation

Initial Model
The first model used a basic Convolutional Neural Network architecture without data augmentation or pre-trained weights. 
It was trained for 10 epochs with the following results:

Training Accuracy: Started at 12.29% and improved to 97.22%.
Validation Accuracy: Started at 36.58% and improved to 62.62%.
Training Loss: Started at 3.2391 and reduced to 0.0878.
Validation Loss: Started at 2.3372 and increased to 2.3744.

Training Log:
Epoch 1/10
2024-05-30 22:11:03.483598: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:15: Filling up shuffle buffer (this may take a while): 135 of 1000
2024-05-30 22:11:11.699752: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.
329/329 ━━━━━━━━━━━━━━━━━━━━ 166s 443ms/step - accuracy: 0.1229 - loss: 3.2391 - val_accuracy: 0.3658 - val_loss: 2.3372
Epoch 2/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 136s 413ms/step - accuracy: 0.4554 - loss: 1.9948 - val_accuracy: 0.5067 - val_loss: 1.8550
Epoch 3/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 138s 421ms/step - accuracy: 0.7163 - loss: 1.0337 - val_accuracy: 0.5649 - val_loss: 1.7648
Epoch 4/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 133s 404ms/step - accuracy: 0.8634 - loss: 0.4826 - val_accuracy: 0.5987 - val_loss: 1.9715
Epoch 5/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 125s 381ms/step - accuracy: 0.9261 - loss: 0.2712 - val_accuracy: 0.6196 - val_loss: 2.0133
Epoch 6/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 129s 391ms/step - accuracy: 0.9498 - loss: 0.1936 - val_accuracy: 0.6173 - val_loss: 2.2705
Epoch 7/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 133s 405ms/step - accuracy: 0.9614 - loss: 0.1374 - val_accuracy: 0.6427 - val_loss: 2.2777
Epoch 8/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 128s 389ms/step - accuracy: 0.9670 - loss: 0.1290 - val_accuracy: 0.6156 - val_loss: 2.4063
Epoch 9/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 126s 384ms/step - accuracy: 0.9682 - loss: 0.1250 - val_accuracy: 0.6351 - val_loss: 2.2292
Epoch 10/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 126s 384ms/step - accuracy: 0.9722 - loss: 0.0878 - val_accuracy: 0.6262 - val_loss: 2.3744

The initial model showed significant overfitting, indicated by the high training accuracy but much lower validation accuracy and increasing validation loss. 
The model learned the training data well but failed to generalize on new data. This overfitting is likely due to the lack of regularization techniques 
such as dropout and data augmentation.

Second Model
The second model includes data augmentation and dropout to reduce overfitting and was trained for 10 epochs:

Training Accuracy: Started at 9.69% and improved to 57.39%.
Validation Accuracy: Started at 22.84% and improved to 49.51%.
Training Loss: Started at 3.2969 and reduced to 1.4541.
Validation Loss: Started at 2.7191 and reduced to 1.9250.

Training Log:
Epoch 1/10
2024-05-30 22:49:56.160637: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:15: Filling up shuffle buffer (this may take a while): 242 of 1000
2024-05-30 22:50:01.614898: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.
329/329 ━━━━━━━━━━━━━━━━━━━━ 198s 543ms/step - accuracy: 0.0969 - loss: 3.2969 - val_accuracy: 0.2284 - val_loss: 2.7191
Epoch 2/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 170s 518ms/step - accuracy: 0.2622 - loss: 2.6203 - val_accuracy: 0.3227 - val_loss: 2.4599
Epoch 3/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 166s 504ms/step - accuracy: 0.3441 - loss: 2.3042 - val_accuracy: 0.3676 - val_loss: 2.2660
Epoch 4/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 166s 506ms/step - accuracy: 0.3940 - loss: 2.1107 - val_accuracy: 0.4009 - val_loss: 2.1362
Epoch 5/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 170s 518ms/step - accuracy: 0.4421 - loss: 1.9241 - val_accuracy: 0.4631 - val_loss: 1.9778
Epoch 6/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 165s 501ms/step - accuracy: 0.4608 - loss: 1.8453 - val_accuracy: 0.4551 - val_loss: 2.0072
Epoch 7/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 168s 509ms/step - accuracy: 0.5038 - loss: 1.7101 - val_accuracy: 0.4884 - val_loss: 1.8923
Epoch 8/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 165s 500ms/step - accuracy: 0.5405 - loss: 1.6031 - val_accuracy: 0.5049 - val_loss: 1.8785
Epoch 9/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 165s 500ms/step - accuracy: 0.5608 - loss: 1.5230 - val_accuracy: 0.5089 - val_loss: 1.8463
Epoch 10/10
329/329 ━━━━━━━━━━━━━━━━━━━━ 166s 505ms/step - accuracy: 0.5739 - loss: 1.4541 - val_accuracy: 0.4951 - val_loss: 1.9250

While data augmentation and dropout helped to reduce overfitting, the overall performance of the model was still not satisfactory. 
The accuracy levels were low, suggesting that the model complexity and training strategy needed further improvement. 
The model struggled to effectively capture the features necessary for accurate classification.

Final Model

The final model utilized MobileNetV2, a pre-trained model on ImageNet, with data augmentation, learning rate scheduling, and early stopping. 
Initial training and fine-tuning were performed over 20 and 30 epochs, respectively.

Initial Training Results:

Training Accuracy: Started at 45.73% and improved to 90.95%.
Validation Accuracy: Started at 70.89% and improved to 84.13%.
Training Loss: Started at 1.9530 and reduced to 0.0331.
Validation Loss: Started at 0.9405 and reduced to 0.6932.

Fine-tuning Results:

Training Accuracy: Started at 69.52% and improved to 99.23%.
Validation Accuracy: Started at 75.64% and improved to 84.13%.
Training Loss: Started at 1.0143 and reduced to 0.0331.
Validation Loss: Started at 0.9374 and reduced to 0.6932.

Training Log:
Epoch 1/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 218s 316ms/step - accuracy: 0.4573 - loss: 1.9530 - val_accuracy: 0.7089 - val_loss: 0.9405 - learning_rate: 0.0010
Epoch 2/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 210s 320ms/step - accuracy: 0.7098 - loss: 0.9372 - val_accuracy: 0.7222 - val_loss: 0.9010 - learning_rate: 0.0010
Epoch 3/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 206s 313ms/step - accuracy: 0.7410 - loss: 0.8056 - val_accuracy: 0.7569 - val_loss: 0.7708 - learning_rate: 0.0010
Epoch 4/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 203s 309ms/step - accuracy: 0.7769 - loss: 0.7007 - val_accuracy: 0.7618 - val_loss: 0.7920 - learning_rate: 0.0010
Epoch 5/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 202s 308ms/step - accuracy: 0.7930 - loss: 0.6307 - val_accuracy: 0.7618 - val_loss: 0.7722 - learning_rate: 0.0010
Epoch 6/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 207s 315ms/step - accuracy: 0.8037 - loss: 0.5910 - val_accuracy: 0.7800 - val_loss: 0.7362 - learning_rate: 0.0010
Epoch 7/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 206s 314ms/step - accuracy: 0.8215 - loss: 0.5247 - val_accuracy: 0.7756 - val_loss: 0.7457 - learning_rate: 0.0010
Epoch 8/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 204s 310ms/step - accuracy: 0.8321 - loss: 0.5051 - val_accuracy: 0.7813 - val_loss: 0.8038 - learning_rate: 0.0010
Epoch 9/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 201s 306ms/step - accuracy: 0.8357 - loss: 0.4785 - val_accuracy: 0.7698 - val_loss: 0.7563 - learning_rate: 0.0010
Epoch 10/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 203s 308ms/step - accuracy: 0.8678 - loss: 0.3904 - val_accuracy: 0.7693 - val_loss: 0.7851 - learning_rate: 5.0000e-04
Epoch 11/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 203s 309ms/step - accuracy: 0.8799 - loss: 0.3598 - val_accuracy: 0.7876 - val_loss: 0.7092 - learning_rate: 5.0000e-04
Epoch 12/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 204s 311ms/step - accuracy: 0.8863 - loss: 0.3294 - val_accuracy: 0.7956 - val_loss: 0.7261 - learning_rate: 5.0000e-04
Epoch 13/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 204s 310ms/step - accuracy: 0.8841 - loss: 0.3343 - val_accuracy: 0.7862 - val_loss: 0.7327 - learning_rate: 5.0000e-04
Epoch 14/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 201s 307ms/step - accuracy: 0.8947 - loss: 0.3132 - val_accuracy: 0.7902 - val_loss: 0.7521 - learning_rate: 5.0000e-04
Epoch 15/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 201s 306ms/step - accuracy: 0.8995 - loss: 0.2912 - val_accuracy: 0.8009 - val_loss: 0.7103 - learning_rate: 2.5000e-04
Epoch 16/20
657/657 ━━━━━━━━━━━━━━━━━━━━ 206s 313ms/step - accuracy: 0.9079 - loss: 0.2696 - val_accuracy: 0.8000 - val_loss: 0.7206 - learning_rate: 2.5000e-04
Epoch 1/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 315s 464ms/step - accuracy: 0.6952 - loss: 1.0143 - val_accuracy: 0.7564 - val_loss: 0.9374 - learning_rate: 1.0000e-04
Epoch 2/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 301s 458ms/step - accuracy: 0.8380 - loss: 0.5077 - val_accuracy: 0.7391 - val_loss: 1.0244 - learning_rate: 1.0000e-04
Epoch 3/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 307s 468ms/step - accuracy: 0.8708 - loss: 0.3746 - val_accuracy: 0.7702 - val_loss: 0.8963 - learning_rate: 1.0000e-04
Epoch 4/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 302s 460ms/step - accuracy: 0.9112 - loss: 0.2559 - val_accuracy: 0.7796 - val_loss: 0.8557 - learning_rate: 1.0000e-04
Epoch 5/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 301s 458ms/step - accuracy: 0.9282 - loss: 0.2057 - val_accuracy: 0.8058 - val_loss: 0.7857 - learning_rate: 1.0000e-04
Epoch 6/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 307s 467ms/step - accuracy: 0.9267 - loss: 0.1940 - val_accuracy: 0.7920 - val_loss: 0.8368 - learning_rate: 1.0000e-04
Epoch 7/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 302s 460ms/step - accuracy: 0.9398 - loss: 0.1679 - val_accuracy: 0.7822 - val_loss: 0.9683 - learning_rate: 1.0000e-04
Epoch 8/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 301s 459ms/step - accuracy: 0.9600 - loss: 0.1223 - val_accuracy: 0.8289 - val_loss: 0.6959 - learning_rate: 5.0000e-05
Epoch 9/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 302s 460ms/step - accuracy: 0.9741 - loss: 0.0744 - val_accuracy: 0.8409 - val_loss: 0.6625 - learning_rate: 5.0000e-05
Epoch 10/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 312s 474ms/step - accuracy: 0.9749 - loss: 0.0755 - val_accuracy: 0.8378 - val_loss: 0.7103 - learning_rate: 5.0000e-05
Epoch 11/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 313s 476ms/step - accuracy: 0.9784 - loss: 0.0648 - val_accuracy: 0.8360 - val_loss: 0.7226 - learning_rate: 5.0000e-05
Epoch 12/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 301s 458ms/step - accuracy: 0.9822 - loss: 0.0559 - val_accuracy: 0.8364 - val_loss: 0.7005 - learning_rate: 2.5000e-05
Epoch 13/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 302s 460ms/step - accuracy: 0.9877 - loss: 0.0431 - val_accuracy: 0.8396 - val_loss: 0.6953 - learning_rate: 2.5000e-05
Epoch 14/30
657/657 ━━━━━━━━━━━━━━━━━━━━ 300s 457ms/step - accuracy: 0.9923 - loss: 0.0331 - val_accuracy: 0.8413 - val_loss: 0.6932 - learning_rate: 1.2500e-05

The use of a pre-trained model significantly improved the models performance. MobileNetV2 provided a strong feature extraction backbone, and fine-tuning allowed 
the model to adapt these features to the specific trash classification task. The introduction of a learning rate scheduler and early stopping also contributed 
to more stable and effective training, preventing overfitting and ensuring the models generalization capability.
